{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5ba47f5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06456325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, unicodedata, math\n",
    "from typing import Dict, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd140f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# NORMALISATION\n",
    "# -------------------------------\n",
    "def strip_accents(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def normalize(s):\n",
    "    s = strip_accents(s.lower().strip())\n",
    "    s = re.sub(r'[^a-z0-9:/ \\-]', ' ', s)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "427c81d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# TOKENISATION LÉGÈRE\n",
    "# -------------------------------\n",
    "def tokenize(s):\n",
    "    return normalize(s).split()\n",
    "\n",
    "# -------------------------------\n",
    "# n-GRAMMES + COSINE\n",
    "# -------------------------------\n",
    "def char_ngrams(s, n=3):\n",
    "    s = f\" {s} \"\n",
    "    grams = {}\n",
    "    for i in range(max(0, len(s)-n+1)):\n",
    "        g = s[i:i+n]\n",
    "        grams[g] = grams.get(g, 0)+1\n",
    "    return grams\n",
    "\n",
    "def cosine(a, b):\n",
    "    keys = set(a.keys()) | set(b.keys())\n",
    "    dot = sum(a.get(k,0)*b.get(k,0) for k in keys)\n",
    "    na = math.sqrt(sum(v*v for v in a.values()))\n",
    "    nb = math.sqrt(sum(v*v for v in b.values()))\n",
    "    return 0 if na==0 or nb==0 else dot/(na*nb)\n",
    "\n",
    "def ngram_score(u, v):\n",
    "    return sum(cosine(char_ngrams(u,n), char_ngrams(v,n)) for n in (3,4,5))/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4f825b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# TOKENISATION LÉGÈRE\n",
    "# -------------------------------\n",
    "def tokenize(s):\n",
    "    return normalize(s).split()\n",
    "\n",
    "# -------------------------------\n",
    "# n-GRAMMES + COSINE\n",
    "# -------------------------------\n",
    "def char_ngrams(s, n=3):\n",
    "    s = f\" {s} \"\n",
    "    grams = {}\n",
    "    for i in range(max(0, len(s)-n+1)):\n",
    "        g = s[i:i+n]\n",
    "        grams[g] = grams.get(g, 0)+1\n",
    "    return grams\n",
    "\n",
    "def cosine(a, b):\n",
    "    keys = set(a.keys()) | set(b.keys())\n",
    "    dot = sum(a.get(k,0)*b.get(k,0) for k in keys)\n",
    "    na = math.sqrt(sum(v*v for v in a.values()))\n",
    "    nb = math.sqrt(sum(v*v for v in b.values()))\n",
    "    return 0 if na==0 or nb==0 else dot/(na*nb)\n",
    "\n",
    "def ngram_score(u, v):\n",
    "    return sum(cosine(char_ngrams(u,n), char_ngrams(v,n)) for n in (3,4,5))/3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adc714f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# SYNONYMES & MOTS CLÉS\n",
    "# -------------------------------\n",
    "SYNONYMS = {\n",
    "    \"patient\": [\"malade\", \"personne\", \"cas\"],\n",
    "    \"chirurgien\": [\"docteur\", \"chirurgie\", \"operateur\"],\n",
    "    \"anesthesiste\": [\"anap\", \"anesth\"],\n",
    "    \"salle\": [\"bloc\", \"piece\", \"chambre\"],\n",
    "    \"heure\": [\"temps\", \"moment\"],\n",
    "    \"site\": [\"zone\", \"endroit\", \"localisation\"],\n",
    "}\n",
    "\n",
    "def replace_synonyms(tokens):\n",
    "    out = []\n",
    "    for t in tokens:\n",
    "        for key, syns in SYNONYMS.items():\n",
    "            if t in syns:\n",
    "                t = key\n",
    "                break\n",
    "        out.append(t)\n",
    "    return out\n",
    "\n",
    "def keyword_overlap(u_tokens, v_tokens):\n",
    "    u_set, v_set = set(replace_synonyms(u_tokens)), set(replace_synonyms(v_tokens))\n",
    "    return len(u_set & v_set) / max(1, len(v_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3811f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# SCORE GLOBAL = pondération lexicale + contextuelle\n",
    "# -------------------------------\n",
    "def robust_similarity(utterance, candidate):\n",
    "    u, c = normalize(utterance), normalize(candidate)\n",
    "    # score de similarité caractères\n",
    "    ngram_sim = ngram_score(u, c)\n",
    "    # similarité mots / synonymes\n",
    "    kw_sim = keyword_overlap(tokenize(u), tokenize(c))\n",
    "    return 0.7*ngram_sim + 0.3*kw_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ef47258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# GÉNÉRATION DES CANDIDATS\n",
    "# -------------------------------\n",
    "def variants_nom(full):\n",
    "    full = normalize(full)\n",
    "    parts = full.split()\n",
    "    var = [full]\n",
    "    if len(parts)==2:\n",
    "        var += [f\"{parts[1]} {parts[0]}\"]\n",
    "    return var\n",
    "\n",
    "def variants_heure(h):\n",
    "    return [h, h.replace(':','h')]\n",
    "\n",
    "def variants_salle(s):\n",
    "    num = re.findall(r'\\d+', s)\n",
    "    if num:\n",
    "        n = num[0]\n",
    "        return [s, f\"salle {n}\", f\"bloc {n}\"]\n",
    "    return [s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "557b46a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# FONCTION PRINCIPALE\n",
    "# -------------------------------\n",
    "def match_json_field(utterance, fiche):\n",
    "    \"\"\"\n",
    "    Compare l'énoncé à tous les champs connus du JSON et renvoie les meilleurs match\n",
    "    \"\"\"\n",
    "    fields = {}\n",
    "    pat = fiche[\"patient\"]\n",
    "    inter = fiche[\"intervention\"]\n",
    "\n",
    "    candidates = {\n",
    "        \"PATIENT_IDENTITE\": variants_nom(f\"{pat['prenom']} {pat['nom']}\"),\n",
    "        \"CHIRURGIEN\": variants_nom(inter[\"chirurgien\"]),\n",
    "        \"ANESTHESISTE\": variants_nom(inter.get(\"anesthesiste\",\"\")),\n",
    "        \"HEURE_PREVUE\": variants_heure(inter[\"heure_prevue\"]),\n",
    "        \"SITE_OPERATOIRE\": [inter[\"site_operatoire\"]],\n",
    "        \"INTERVENTION_TYPE\": [inter[\"type\"]],\n",
    "        \"SALLE\": variants_salle(inter.get(\"bloc\",\"\"))\n",
    "    }\n",
    "\n",
    "    best_field, best_score, best_value = None, 0, \"\"\n",
    "    for field, vals in candidates.items():\n",
    "        for v in vals:\n",
    "            s = robust_similarity(utterance, v)\n",
    "            if s > best_score:\n",
    "                best_field, best_score, best_value = field, s, v\n",
    "\n",
    "    decision = \"OK\" if best_score >= 0.88 else (\"INCERTAIN\" if best_score >= 0.70 else \"KO\")\n",
    "    return {\n",
    "        \"utterance\": utterance,\n",
    "        \"field\": best_field,\n",
    "        \"value\": best_value,\n",
    "        \"score\": round(best_score,3),\n",
    "        \"decision\": decision\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e0f8193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'utterance': \"aujourd'hui on opère Paul Dupont\", 'field': 'PATIENT_IDENTITE', 'value': 'paul dupont', 'score': 0.697, 'decision': 'KO'}\n",
      "{'utterance': \"le patient c'est monsieur Dupont Paul\", 'field': 'PATIENT_IDENTITE', 'value': 'dupont paul', 'score': 0.684, 'decision': 'KO'}\n",
      "{'utterance': 'on démarre à 10h30', 'field': 'HEURE_PREVUE', 'value': '10h30', 'score': 0.637, 'decision': 'KO'}\n",
      "{'utterance': 'bloc numéro 3 prêt', 'field': 'SALLE', 'value': 'bloc 3', 'score': 0.547, 'decision': 'KO'}\n",
      "{'utterance': 'le docteur lefevre est là', 'field': 'CHIRURGIEN', 'value': 'dr lefevre', 'score': 0.492, 'decision': 'KO'}\n",
      "{'utterance': 'site fosse droite', 'field': 'SITE_OPERATOIRE', 'value': 'Fosse iliaque droite', 'score': 0.612, 'decision': 'KO'}\n",
      "{'utterance': 'appendice prévu', 'field': 'INTERVENTION_TYPE', 'value': 'Appendicectomie', 'score': 0.349, 'decision': 'KO'}\n"
     ]
    }
   ],
   "source": [
    "fiche = {\n",
    "  \"patient\": {\"prenom\":\"Paul\",\"nom\":\"Dupont\",\"date_naissance\":\"1975-03-12\"},\n",
    "  \"intervention\": {\n",
    "    \"type\":\"Appendicectomie\",\n",
    "    \"heure_prevue\":\"10:30\",\n",
    "    \"site_operatoire\":\"Fosse iliaque droite\",\n",
    "    \"chirurgien\":\"Dr. Lefèvre\",\n",
    "    \"anesthesiste\":\"Dr. Bernard\",\n",
    "    \"bloc\":\"Salle 3\"\n",
    "  }\n",
    "}\n",
    "\n",
    "tests = [\n",
    "    \"aujourd'hui on opère Paul Dupont\",\n",
    "    \"le patient c'est monsieur Dupont Paul\",\n",
    "    \"on démarre à 10h30\",\n",
    "    \"bloc numéro 3 prêt\",\n",
    "    \"le docteur lefevre est là\",\n",
    "    \"site fosse droite\",\n",
    "    \"appendice prévu\",\n",
    "]\n",
    "for t in tests:\n",
    "    print(match_json_field(t, fiche))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f7d158",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "altusafe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
